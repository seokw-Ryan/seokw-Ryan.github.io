<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>compass-cli - Ryan Chung</title>
  <style>
    :root {
      --bg-color: #FDFBF7;
      --text-color: #000000;
      --accent-color: #333333;
      --font-serif: "Times New Roman", Times, serif;
      --font-mono: "Courier New", Courier, monospace;
    }

    html, body {
      margin: 0;
      padding: 0;
      background-color: var(--bg-color);
      color: var(--text-color);
      font-family: var(--font-mono);
      line-height: 1.6;
    }

    body {
      padding: 20px;
      max-width: 900px;
      margin: 0 auto;
      min-height: 100vh;
      overflow-y: auto;
      overflow-x: hidden;
      display: flex;
      flex-direction: column;
    }

    h1, h2, h3 {
      font-family: var(--font-serif);
      color: var(--text-color);
    }

    h1 {
      margin-top: 1rem;
      font-size: 1.8rem;
      text-align: center;
    }

    .project-date {
      text-align: center;
      margin-top: 0.3rem;
      margin-bottom: 1.5rem;
      color: rgba(0, 0, 0, 0.7);
    }

    .project-box {
      border: 1px solid #000;
      padding: 1.5rem;
      margin-top: 1.5rem;
    }

    .project-box h2 {
      font-size: 1.3rem;
      margin-top: 2rem;
      margin-bottom: 0.8rem;
    }

    .project-box h2:first-child {
      margin-top: 0;
    }

    p {
      margin-bottom: 1.5rem;
      font-size: 1.05rem;
    }

    ul {
      padding-left: 1.5rem;
      margin-bottom: 1.5rem;
    }

    li {
      margin-bottom: 0.5rem;
      font-size: 1.05rem;
    }

    .tech-badges {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-bottom: 1.5rem;
    }

    .tech-badges span {
      padding: 0.2rem 0.6rem;
      border: 1px solid rgba(0, 0, 0, 0.3);
      font-size: 0.85rem;
      color: rgba(0, 0, 0, 0.7);
    }

    .nav-buttons {
      display: flex;
      flex-direction: row;
      flex-wrap: wrap;
      gap: 1rem;
      justify-content: center;
      align-items: center;
      padding: 0.5rem;
      width: 100%;
      margin-top: auto;
      margin-bottom: 1rem;
    }

    .nav-buttons a {
      text-decoration: none;
      font-size: clamp(0.8rem, 1.8vw, 1rem);
      color: var(--text-color);
      border: 1px solid #000;
      padding: 0.5rem 1rem;
      white-space: nowrap;
      background-color: var(--bg-color);
      transition: all 0.2s ease;
      width: 220px;
      text-align: center;
      box-sizing: border-box;
    }

    .nav-buttons a:hover {
      background-color: #000;
      color: var(--bg-color);
    }

    @media (max-width: 600px) {
      .nav-buttons {
        gap: 0.5rem;
      }

      .nav-buttons a {
        width: 160px;
        font-size: clamp(0.7rem, 1.5vw, 0.9rem);
      }

      h1 {
        font-size: 1.6rem;
      }
    }
  </style>
</head>
<body>
  <main style="flex: 1;">
    <h1>compass-cli</h1>
    <div class="project-date">January 2026 &mdash; Present</div>
    <div class="tech-badges">
      <span>Python</span>
      <span>PyTorch</span>
      <span>FastAPI</span>
      <span>Next.js</span>
      <span>React</span>
      <span>SQLite</span>
      <span>PostgreSQL</span>
      <span>Docker</span>
      <span>Typer</span>
      <span>Pydantic</span>
    </div>
    <article class="project-box">
      <p>
        compass is a local-first command-line tool for personal knowledge
        management with RAG (retrieval-augmented generation) capabilities.
        It lets you initialize a vault&mdash;a regular directory on your
        machine&mdash;ingest documents into it, and then query your own
        knowledge base through an interactive chat interface backed by
        language models. All data stays local by default; conversation
        history is never sent to external APIs.
      </p>

      <h2>Motivation</h2>
      <p>
        There is no shortage of ChatGPT wrappers. The barrier to shipping
        an AI-powered product has dropped to the point where most of them
        are thin skins over an API call&mdash;prompt in, completion out,
        deploy to Vercel, done. The result is a flood of nearly identical
        apps that outsource every hard problem to a single provider and
        call it a day.
      </p>
      <p>
        I wanted to go in the opposite direction: build a full-stack AI
        application end to end, from training and fine-tuning models with
        PyTorch and LoRA, to implementing the retrieval and embedding
        pipeline, to standing up the backend (FastAPI, PostgreSQL with
        pgvector), the frontend (Next.js), and the infrastructure
        (Docker, Nginx) for actual cloud deployment. Not because wrapping
        an API is wrong, but because I wanted to understand every layer
        of the stack myself&mdash;how embeddings are generated and stored,
        how hybrid search and reranking work under the hood, how a model
        serving pipeline fits together, and what it actually takes to go
        from a training script to a running service. compass is the
        vehicle for that.
      </p>

      <h2>Local-First Design</h2>
      <p>
        Vaults are ordinary directories with a <code>.compass/</code>
        subdirectory that holds a SQLite database for document chunks and
        embeddings, a profile config, and optional custom slash commands.
        Application config follows the XDG Base Directory Specification
        (TOML at <code>~/.config/compass/config.toml</code>), and chat
        sessions are persisted as JSON files under
        <code>~/.local/state/compass/sessions/</code>. There is no
        telemetry, no cloud sync, and no account required. Vaults can be
        backed up or moved like any other folder.
      </p>

      <h2>Document Ingestion and RAG</h2>
      <p>
        The ingestion pipeline loads documents from multiple formats
        (Markdown, plain text, logs), chunks them for retrieval, and
        stores the chunks with embeddings in the vault's SQLite database.
        At query time, compass retrieves relevant chunks, optionally
        re-ranks them, and sends only the current query plus the retrieved
        context to the configured LLM provider&mdash;keeping the full
        conversation history local.
      </p>

      <h2>LLM Providers</h2>
      <p>
        compass supports multiple LLM providers: Ollama for fully local
        inference, plus OpenAI, Anthropic, and Google for cloud-backed
        queries. The default is local (Ollama), and when cloud providers
        are used, only the current prompt and RAG context leave the
        machine. Provider selection is handled through the TOML config
        with dot-notation key access.
      </p>

      <h2>CLI and Distribution</h2>
      <p>
        The CLI is built on Typer with Rich for terminal formatting.
        Core commands include <code>init</code> (create a vault),
        <code>config</code> (manage settings), <code>ingest</code>
        (add documents), <code>chat</code> (interactive session with
        resume), and <code>exec</code> (one-off prompts). The tool ships
        as both a Python package and a Node.js wrapper for npm
        distribution, with platform-aware binary detection for Linux,
        macOS, and Windows.
      </p>

      <h2>What's Next</h2>
      <ul>
        <li>Full LLM provider implementations beyond the current interface stubs</li>
        <li>Complete RAG pipeline with embedding generation and re-ranking</li>
        <li>Broader document format support (PDF, HTML, EPUB)</li>
        <li>Decision journaling and weekly review workflows</li>
      </ul>
    </article>
  </main>
  <nav class="nav-buttons">
    <a href="../projects.html">Projects</a>
    <a href="../index.html">Home</a>
  </nav>
</body>
</html>
