<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>vla-video-data - Ryan Chung</title>
  <style>
    :root {
      --bg-color: #FDFBF7;
      --text-color: #000000;
      --accent-color: #333333;
      --font-serif: "Times New Roman", Times, serif;
      --font-mono: "Courier New", Courier, monospace;
    }

    html, body {
      margin: 0;
      padding: 0;
      background-color: var(--bg-color);
      color: var(--text-color);
      font-family: var(--font-mono);
      line-height: 1.6;
    }

    body {
      padding: 20px;
      max-width: 900px;
      margin: 0 auto;
      min-height: 100vh;
      overflow-y: auto;
      overflow-x: hidden;
      display: flex;
      flex-direction: column;
    }

    h1, h2, h3 {
      font-family: var(--font-serif);
      color: var(--text-color);
    }

    h1 {
      margin-top: 1rem;
      font-size: 1.8rem;
      text-align: center;
    }

    .project-box {
      border: 1px solid #000;
      padding: 1.5rem;
      margin-top: 1.5rem;
    }

    .project-box h2 {
      font-size: 1.3rem;
      margin-top: 2rem;
      margin-bottom: 0.8rem;
    }

    .project-box h2:first-child {
      margin-top: 0;
    }

    p {
      margin-bottom: 1.5rem;
      font-size: 1.05rem;
    }

    ul {
      padding-left: 1.5rem;
      margin-bottom: 1.5rem;
    }

    li {
      margin-bottom: 0.5rem;
      font-size: 1.05rem;
    }

    .tech-badges {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin-bottom: 1.5rem;
    }

    .tech-badges span {
      padding: 0.2rem 0.6rem;
      border: 1px solid rgba(0, 0, 0, 0.3);
      font-size: 0.85rem;
      color: rgba(0, 0, 0, 0.7);
    }

    .nav-buttons {
      display: flex;
      flex-direction: row;
      flex-wrap: wrap;
      gap: 1rem;
      justify-content: center;
      align-items: center;
      padding: 0.5rem;
      width: 100%;
      margin-top: auto;
      margin-bottom: 1rem;
    }

    .nav-buttons a {
      text-decoration: none;
      font-size: clamp(0.8rem, 1.8vw, 1rem);
      color: var(--text-color);
      border: 1px solid #000;
      padding: 0.5rem 1rem;
      white-space: nowrap;
      background-color: var(--bg-color);
      transition: all 0.2s ease;
      width: 220px;
      text-align: center;
      box-sizing: border-box;
    }

    .nav-buttons a:hover {
      background-color: #000;
      color: var(--bg-color);
    }

    @media (max-width: 600px) {
      .nav-buttons {
        gap: 0.5rem;
      }

      .nav-buttons a {
        width: 160px;
        font-size: clamp(0.7rem, 1.5vw, 0.9rem);
      }

      h1 {
        font-size: 1.6rem;
      }
    }
  </style>
</head>
<body>
  <main style="flex: 1;">
    <h1>vla-video-data</h1>
    <div class="tech-badges">
      <span>Python</span>
      <span>PyTorch</span>
      <span>FFmpeg</span>
      <span>OpenCV</span>
      <span>Hugging Face</span>
      <span>DROID</span>
    </div>
    <article class="project-box">
      <p>
        Video data collection and processing pipeline for training and
        fine-tuning vision-language-action models. VLAs learn manipulation
        and navigation policies from paired video-language-action data, and
        the quality of that data directly determines model performance.
        This project builds the tooling to gather, process, and curate
        video datasets suitable for VLA training.
      </p>

      <h2>Motivation</h2>
      <p>
        Training a VLA from scratch or fine-tuning an existing one on new
        tasks requires large volumes of video paired with language
        instructions and action labels. Off-the-shelf datasets cover a
        limited set of environments and embodiments. To push the
        vla-world-model-control project further, a dedicated data pipeline
        is needed&mdash;one that can ingest video from simulation recordings,
        real-world demonstrations, and public robotics datasets, then
        normalize everything into a consistent format for training.
      </p>

      <h2>Video Collection</h2>
      <p>
        The pipeline collects video from multiple sources: task recordings
        from NVIDIA Isaac Sim, publicly available robotics demonstration
        datasets (such as DROID, Open X-Embodiment, and Bridge V2), and
        manually recorded demonstrations. Each video is paired with
        natural-language task descriptions and, where available,
        action trajectories (end-effector poses, joint angles, or
        discrete actions).
      </p>

      <h2>Processing and Annotation</h2>
      <p>
        Raw video goes through a processing pipeline that handles frame
        extraction, resolution normalization, and temporal subsampling to
        match the input requirements of target VLA architectures. Action
        labels are aligned to video frames, and language annotations are
        validated for consistency. The pipeline outputs training-ready
        data in formats compatible with standard VLA training codebases.
      </p>

      <h2>Dataset Curation</h2>
      <p>
        Not all data is equally useful. The curation stage filters for
        task diversity, demonstration quality, and action coverage.
        Duplicate or near-duplicate episodes are removed, failed
        demonstrations are flagged, and the dataset is balanced across
        task types and difficulty levels to avoid training biases.
      </p>

      <h2>What's Next</h2>
      <ul>
        <li>Build the ingestion pipeline for Isaac Sim task recordings</li>
        <li>Integrate public robotics datasets (DROID, Open X-Embodiment) into the unified format</li>
        <li>Implement frame-action alignment and temporal subsampling</li>
        <li>Run first fine-tuning experiments on a baseline VLA with the curated dataset</li>
      </ul>
    </article>
  </main>
  <nav class="nav-buttons">
    <a href="../projects.html">Projects</a>
    <a href="../index.html">Home</a>
  </nav>
</body>
</html>
